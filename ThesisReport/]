\chapter{Design of Evaluation Scheme}
\label{chap:System}

This chapter walks through the steps taken in order to build up our evaluation framework and justification of each decision taken during this design.

\section{Stereo Correspondence Evaluation}

As briefly mentioned in \ref{chap:Introduction}, there are already different evaluation schemes to assess stereo correspondence solutions. Middlebury Stereo Evaluation \cite{mideval} and 
Kitti Stereo Evaluation \cite{kitti} are the most popular evaluation schemes among researchers and scientists in the field. 
Middlebury and Kitti evaluation schemes are both based on defining certain pixel thresholds for the error between the ground truth disparity and the disparity found by the solution. 
They also provide separate masks for depth discontinuities, occluded regions and the mask of all the area in the image for seperate error and outlier calculation.
However, both of these widely used evaluation models take a general approach towards assessing stereo matching algorithms and have not been designed based on any particular application in which 
stereo matching solutions might be used; therefore, they focus more on the fundamental aspects of designing a stereo matching algorithm as a solution per se to \textit{efficiently}
find the \textit{best matches} of corresponding pixels in stereo pairs. 
However, this perspective may raise some questions a punctilious researcher such as:

\begin{enumerate}
\item What is actually an \textit{efficient} solution? On what basis this \textit{efficienly} can be defined?
\item What is a \textit{best match} of corresponding pixels? How can it be defined?
\end{enumerate}

In fact, these questions have mainly motivated us to study the evaluation scheme of stereo correspondence solutions from a different point of view. 
In this approach, we have taken steps towards evaluating the stereo correspondence solutions based on the application in which they are going to be used, 
thus better defining and tuning the criteria for \textit{efficiency} and 
finding \textit{the best correspondence match} in the evaluation. Since augmented reality applications have more attracted the researchers's attention in the past few years, 
we have decided to build our evaluation scheme based on an outdoor application in this area.

\section{New Evaluation Scheme}

In an augmneted reality system, there are certain factors that would affect the functionality and effectiveness of the system \cite{dras96,liv05}. These factors may correspond to the surrounding 
enivornment, technology and hardware constraints, or human factors. Therefore, in the evaluation of an augmented reality system 
which employs stereo vision techniques to build a depth map of the surrounding environment, these factors should be carefully considered. 
However, due to time constraint and existence of many factors that need to be thoroughly 
examined while evaluating the functionality and efficiecy of an AR system, we have mainly focused on one of the significant aspects of human factors, 
and the requirement of providing a real-time interaction for
the users, which addresses one of the problems related to technology constraints. 

In order to design an evaluation scheme for assessing stereo matching solutions in an AR application, we need to primarily understand the relation between
these two components and the aspects in which they would affect each other while building a 
practical AR system. To better illustrate this relation, a high level architecture of a 3D AR system is shown in \ref{fig:AR}.
\textbf{FIG of AR}

Concentrating on the requirements of providing a real-time interaction between an AR system and the users, along with
certain human factors that would affect the functionality of the system,
has revealed the necessity of involving them in the evaluation of the stereo correspondence methods. Therefore, in order to 
determine whether a stereo correspondence algorithm can meet the requirements of an AR application, we need an evaluation scheme which can properly assess 
the \textit{efficiency} of the algorithm and the accuracy of its disparity results based on specific human factors in binocular vision and augmented reality.
These factors are in fact the concepts related to real-time responsiveness of an AR system mentioned in \ref{chap:Introduction}, and binocular vision, stereopsis, human perception of depth, 
and stereoacuity as thoroughly described in \ref{chap:BinocularVision}.

In other words, we have proposed and desgined an evaluation scheme that studies some of the most important aspects of a system that consists of both
AR and stereo vision components, thus enabling the designers to better evaluate the stereo correspondence solutions that will be intergrated 
in the system.

As a result, unlike Middlebury or Kitti evaluation schemes, we label a pixel in the disparity result as an \textit{outlier} if the corresponding stereoacuity
of the depth error between the ground truth depth and the depth value found by the algorithm is more than the standard stereoacuity 
for human visual system determined
by standard stereo tests. 
Moreover, since we have used the average stereoacuity for different age groups \cite{garn06} in our design, we are able to determine the performance of the algorithm for users 
at different age ranges. This comparison
makes the evaluation results more reliable and applicable to practical applications of AR.
In terms of evaluating the efficiency of the algorithms to investigate whether they meet the requirements for being part of a real-time application, 
we have also integrated the estimation of the average execution time over the input dataset in the evaluation process.
Another unique feature in our evaluation model is the introduction and integration of a masking process that is based on particular regions
which are more important to human visual system, as described in chapter \ref{chap:Intirduction}, in an AR application, and 
therefore, having the option of exclusively applying the evaluation process on those regions in the disparity map rather than the whole image.
The difference and result of doing this type of masking and evaluation is described in more detail in the next chapter.

\textbf{Begining of evaluation chapter?} It should be noted that the input dataset we have used for evaluating two sample 
stereo matching algorithms mentioned in \ref{chap:Intorduction}
to present the results of our evaluation and how they should be interpreted, 
are selected from Kitti Stereo Dataset.
We have chosen Kitti Stereo set because it consists of sample stereo images and ground truth disparity maps
taken from outdoor scenes under real circumstances. \textbf{Begining of evaluation chapter?}



\subsection{Design Overview}
A block diagram of our evaluation system is shown in \ref{fig:architecture}.

\textbf{FIG of Architecture}

As it can be seen, in our evaluation model, the input data consisting of stereo images, ground truth disparity and calibration data should be first passed in. 
Afterwards, defined masks can be generated using \textit{Canny Edge Detector} and \textit{Dilation} operation with appropriate parameters 
selected by the user.
A call to the stereo algorithm, that is being evaluated, with required parameters will result in the generation and storage of the corresponding disparity maps
in the system for the next stage of evaluation. 
When the generation of disparity maps is finished, a call to the evaluation module with the required and optional parameters applies the main evaluation
process and outputs the results of evaluation. After all, different plots can be generated 
over the output data for further assessments according to the paramaters and settings passed in. Plots and output data can then be evaluated and 
corresponding decisions can be made about the solutions that were being tested in the defined framework.

\subsection{Modules}
Our evaluation system consists of the following modules:

\begin{itemize}
\item Input Stereo pairs, caliberation data, ground truth disparity
\item Detected edge masks in the disparity image
\item Masked ground-truth (occluded or non-occluded based on the parameters passed in)
\item Full and masked disparity maps generated by the algorithm 
\item Evaluation method 
\item Output luation results
\end{itemize}

The creation of these components is essential depending on the parameters passed in to the main module to successfully conduct 
the evaluation process.

\subsection{Platform, Technology}
The evaluation process have been implemented on a Unix-based platform using C++ programming language as the language used for implementing 
the core functions of the evaluation and Tool Command Language (TCL) for all the scripts.
