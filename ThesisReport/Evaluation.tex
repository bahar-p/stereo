\chapter{Evaluation}
\label{chap:Evaluation}
\renewcommand{\arraystretch}{0.5}

In this chapter, we will go through our experimental hypotheses, testing scenarios, experiments conducted on two sample 
stereo matching algorithms, SGBM and ADCensus, and the results with our
proposed evaluation system to assess the benefits of using our evaluation model for outdoor AR applications over the general-purpose evaluation models; 
the Middlebury and Kitti Stereo Evaluation.

\section{Stereo Dataset}
It should be noted that the stereo images we have used to conduct the experiments on stereo algorithms in our system,
are selected from the Kitti Stereo Dataset.
In contrary to the Middlebury dataset, the Kitti Stereo Project provides stereo images and ground truth disparity maps
that are taken from outdoor scenes under real circumstances. This property of sample images makes them more appropriate 
for evaluating the performance of the algorithms in outdoor AR applications, thus better meeting the objectives of this study.
We have selected 52 image pairs from the Kitti Stereo dataset based on different photometric and visual properties that are important
in stereo vision and an AR application. Some 
of these properties are listed as follows:
\begin{itemize}
\item Light and shading, that is the scenes including bright, dim, and dark regions
\item Various depth ranges, that is including near field, medium field and far field objects  
\item Depth discontinuity and occlusion
\item Well textured and not properly textured regions
\end{itemize}


\section{Methodology}

Before going through the explanation of the experiments to assess our evaluation model, we restate our main research question in this
study to better justify our hypotheses and the experiments defined for their validation. 
As mentioned earlier in chapter \ref{chap:Introduction}, our main objective is to investigate whether using 
stereo matching techniques to generate the depth map of the 
surrounding environment in an outdoor AR application can meet the requirements of the AR system. 
Therefore, our experiments focus 
on assessing those aspects of our evaluation model that assist to better answer this question.
As a result, our first attempt towards evaluating our model is to investigate and demonstrate whether the results of the evaluation process 
are properly measured and presented in the framework of the important factors in an outdoor AR application.
After confirming this property, which is the key property of our model, we investigate the effect of our proposed masking 
approach on the evaluation results. Moreover, we present how the methods are evaluated in the framework of 
real-time interactive AR systems.
We also explain how the evaluation and comparison of the methods is done in our model with some
experiments on the sample stereo matching algorithms.

\section{Hypotheses}

We have defined a set of hypotheses to evaluate our proposed design. These hypotheses are as follows:

\begin{itemize}
\item \textbf{Hypothesis 1}: \emph{Our model evaluates and demonstrates 
the performance of the stereo matching algorithm in the framework of 
the outdoor augmented reality applications.} 
Unlike the Middlebury and Kitti benchmarks which are considered general-purpose evaluation models, 
our system can particularly evaluate the algorithms in the framework of an 
outdoor augmented reality application to facilitate the process of determining
the proper method for using in the AR system for a high quality real-time generation of the depth map of the surrounding environment from 
the user's point of view.

\item \textbf{Hypothesis 2:} \emph{Observing, evaluating, and consequently 
refining the areas near the depth edges in an image are more important in an AR application.}
Salient edges caused by depth discontinuities, 
which can also represent the object boundaries and occlusion, are one of the 
most important depth cues that helps the observer to better perceive the depth of different objects in the scene. In other words, the areas near the edges corresponding to depth discontinuities
in a scene are more important to the human visual system for perception of depth in an AR application and, therefore, the disparity 
errors in these regions can be detected easier by the HVS. Therefore, we argue that in our model, which has the property of masking and evaluating the results for
these particular regions,
the evaluation results can be of great value to an outdoor AR application.

\item \textbf{Hypothesis 3:} \emph{Our system is better than other evaluation models for assessing the performance of the algorithm in
real-time AR applications.}
Other evaluation models, the Kitti and Middlebury benchmarks, do not evaluate and report on the efficiency of the algorithms
with respect to their execution time. On the other hand, our system is capable of examining and evaluating an algorithm 
based on its execution time and, therefore, can report on its efficiency for real-time AR applications.

\item \textbf{Hypothesis 4:} \emph{The trade-off between the accuracy and the running time of the stereo algorithms can be effectively evaluated 
in the framework of an outdoor AR application through our system.} 
Nearly all the solutions to the problem of stereo correspondence have been dealing with the trade-off between the accuracy of the results and the running time.
Therefore, most of the solutions focus only on improving one of these aspects in the final results. Some methods use certain post processing techniques to refine the 
disparity results in the end, thus improving the accuracy, whereas the others propose particular approaches that can be implemented on the GPU.
Due to the importance of both metrics in an outdoor AR application, we argue that the trade-off between these metrics can be effectively analyzed in our evaluation system.


\end{itemize}

The experiments designed to validate these hypotheses are explained in the following sections.

\section{Experimental Environment and Settings}
Experiments were carried out on a Linux machine with Intel Core(TM) i7 3.20GHz CPU. 
Although ADCensus is proposed as a GPU-based solution to the problem of stereo correspondence, 
we have used the CPU implementation of both algorithms in all the experiments.
The set of parameters used at different steps of the evaluation are presented in the following sections.
It should be noted that these parameters were kept constant for all the images and experiments. However, if a parameter is changed during an experiment for specific
reasons, it will be explicitly mentioned in the description of the experiment.

\subsection{Masking}
In order to build the masks in our system, the OpenCV Canny edge detector and Dilation are used.
Canny have been used to detect the depth edges in the ground truth disparity map, and the dilation operation 
for expanding the detected edge regions in the masking process. The extent to which the regions are expanded
is determined by the number of iterations in the dilation operation. Table \ref{tab:candilparam} shows the parameters used in the Dilation
and the Canny edge detection. However, the \textit{minimum threshold} in Canny is tuned and selected separately for each image 
since the threshold should change depending on the scene.

{\footnotesize
\begin{minipage}{\linewidth}
\begin{center}
\captionof{table}{Masking Parameters}
\label{tab:candilparam}
\begin{tabular}{ |c|c| }
\hline
Dilation\_iterations & 10 \\  \hline
Canny\_apertureSize & 3 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline
}

The ground truth disparity maps in the Kitti stereo dataset are generated by a 3D laser scanner, thereby resembling
a point cloud map of discrete disparity values. This property of the disparity images 
can be problematic for the masking process since it can result in many small streaks as the edges.
Therefore, before applying any edge
detection on the image, we need to first fill the gaps by interpolating the values and obtain a smoothed ground truth disparity.
This can be achieved by applying a dilation operation.
In our implementation, we have used the OpenCV dilation operation with different number of iterations for each image, that is set depending on the scene 
and the original ground truth disparity, to obtain a fully dense disparity map. 
The new disparity images are then stored on the disk for further use.
However, it should be noted that the dilated disparity images are only used in the construction of the masks when detecting the depth
edges in the image.
%However, it should be noted that the ground truth disparities used for the related comparisons and calculations 
%in the evaluation process, are the original disparity images before being dilation.

\subsection{Stereo Algorithms Settings}
The parameters for each algorithm used in our experiments to generate the disparity
maps are kept constant over all the images in the dataset. These parameters are presented in Tables \ref{tab:sgbmparams} and \ref{tab:adcparams} 
for SGBM and ADCensus, respectively.

{\footnotesize
\begin{minipage}{\linewidth}
\begin{center}
\captionof{table}{SGBM Parameters}
\label{tab:sgbmparams}
\begin{tabular}{ |c|c|c|c|}
\hline
SADWindowSize & 9 & disp12MaxDiff & 2 \\ \hline
uniquenessRatio & 10 & P2 & 3*9 \\ \hline
speckleWindowSize & 100 & speckleRange & 2 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline
}

Other parameters not mentioned in the table are considered with their default values.

{\footnotesize
\begin{minipage}{\linewidth}
\begin{center}
\captionof{table}{ADCensus Parameters}
\label{tab:adcparams}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
$\lambda_{AD}$ & 10 & $\lambda_{Census}$ & 30 & $L_{1}$ & 34 & $L_{2}$ & 17 \\ \hline
$\tau_{1}$ & 20 & $\tau_{2}$ & 6 & $\pi_{1}$ & 1.0 & $\pi_{2}$ & 3.0 \\ \hline 
$\tau_{SO}$ & 15 & $\tau_{S}$ & 20 & $\tau_{H}$ & 0.4 & & \\  \hline
\end{tabular}
\end{center}
\end{minipage} \newline
}

The minimum and maximum disparity values are also kept constant for each image pair in both algorithms; however, the maximum 
disparity differ for each image pair as the scenes are different
and objects are located at different depth fields.
The minimum disparity is set to $0$ for both algorithms. The maximum disparity for each image pair is selected based on the maximum value in their
corresponding ground truth disparity. The only restriction to consider here is to choose a value greater than or equal to 
the maximum disparity of the ground truth that is a multiplication of 16. This constraint
is implied by the implementation of SGBM algorithm.

\subsection{Evaluation Params}
In our evaluation model, due to the large amount of data which grows as more images are added to the input selection, 
plots are generated by taking the average of the results over all the images. As mentioned in the previous chapter, this average results from two steps; 
first, getting the average of the stereoacuity over specific
depth ranges for each image and then getting the average of the values from the previous step over all of the images. This operation finally results in a single plot
that demonstrates the average stereoacuity within specific distance.
The averaging operations at this step are implemented by building histogram over the resulting data. 
In our experiments, we set the number of bins to $100$ and the width of each bin to $0.5$. Therefore, the first averaging is conducted over distances of $0.5m$ 
in each image and the maximum distance over which the results are 
examined is $50m$.

%\section{Assumption}
%\textbf{Talk about camera quality and display resolution}

\section{Experiments}
In this section, we discuss the experiments conducted to evaluate the system and investigate the validity of our hypotheses.

\subsection{Evaluation in Augmented Reality Framework}

In this experiment, the disparity maps 
were generated for fifty-two image pairs with both SGBM and ADCensus algorithms. 
After generating the corresponding disparity maps for all the images, 
the evaluation process was conducted on each map separately.

Sample plots corresponding to one of the stereo pairs, shown in Figure \ref{fig:img5},
over the masked areas are displayed in Figures \ref{fig:imgmsk5} and \ref{fig:imgfull5}, respectively.

\begin{figure}[h!]
\centering
\subcaptionbox{Left image}
[.5\linewidth]{\includegraphics[scale=0.21]{000005L}}%
\subcaptionbox{Right image}
[.5\linewidth]{\includegraphics[scale=0.21]{000005R}}%
\caption{Sample stereo image from the Kitti dataset}
\label{fig:img5}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{sgbmimg5pix3msk}
\caption{Average disparity error over distance by SGBM}
\label{fig:imgmsk5}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcimg5pix3msk}
\caption{Average disparity error over distance by ADCensus}
\label{fig:imgfull5}
\end{figure}

\noindent
The corresponding mask, Figure \ref{fig:msk}; masked ground truth, Figure \ref{fig:gtmsk}; and
the masked disparity images generated by SGBM and ADCensus, Figures \ref{fig:5mdispsgb} and \ref{fig:5mdispadc} 
are shown below.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{5msk}
\caption{The mask of depth edges and their surrounding regions}
\label{fig:msk}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{5gt}
\caption{Masked ground truth}
\label{fig:gtmsk}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{5mdispsgb}
\caption{Masked disparity by SGBM}
\label{fig:5mdispsgb}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{5mdispadc}
\caption{Masked disparity by ADCensus}
\label{fig:5mdispadc}
\end{figure} 

\noindent
Figures \ref{fig:mskmapsgbm} and \ref{fig:mskmapadc} show the average results over all the disparity images for both SGBM and ADCensus, respectively.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{sgbmmsk1000}
\caption{Average disparity error over all the images by SGBM}
\label{fig:mskmapsgbm}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcenmsk1000}
\caption{Average disparity error over all the images by ADCensus}
\label{fig:mskmapadc}
\end{figure} 

As can be seen, the average results displayed in the previous plots contain sparse points and 
do not demonstrate any consistent pattern. When we investigated the cause of this large variation, we found that in
the results of both algorithms, there are some disparity values which differ from the ground truth 
by a considerable amount and yet have not been invalidated by the
algorithm. We assume that these types of outliers can be easily removed from the set by applying a post processing filter, or 
they will be eventually culled out by the 3D renderer in the AR system. 
%Therefore, we have added another filtering condition to our evaluation module is similar to the approach employed by the
%Kiiti and Middlebury stereo evaluation. 
In order to filter out the disparity values which largely differ from the ground truth disparity, we have integrated another 
step in our evaluation process. This step is similar to the strategy used in the Kitti and Middlebury evaluation models.
In this step, the estimated disparity error is initially compared to a more generally defined threshold, for instance a threshold of 3 pixels.
This comparison allows for only those values of disparity with an error less than or equal to the specified threshold to 
move on to the next steps of the evaluation. It should be noted in our design, the specified threshold is defined as a run-time variable. 

The additional filtering had a significant impact on the evaluation results. In fact, a consistent pattern was observed in the final plots after filtering out the
outliers with large differences. The results are displayed in
figures \ref{fig:mskmapsgbm} and \ref{fig:mskmapadc}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{sgbmmsk3}
\caption{Average disparity error over all the images by SGBM}
\label{fig:mskmapsgbm}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcenmsk3}
\caption{Average disparity error over all the images by ADCensus}
\label{fig:mskmapadc}
\end{figure} 

In these plots, a cross point below a stereoacuity threshold (straight lines) implies that the average error in the disparity values estimated 
by the stereo matching 
algorithm is imperceptible to the human visual system. However, a value higher than the threshold indicates that
the error cannot be ignored and should be resolved to achieve a better alignment between the virtual and the 
real world in the AR application of interest. Moreover, as can be seen most of the errors
fall below the standard stereoacuity value corresponding to older ages; indicating that they are not perceptible to the visual system of the people at these 
particular ages.

The zero values in the plots imply that either there is no object within the corresponding range or the disparity value estimated by the algorithm
is equal to the ground truth disparity; however, since the average of the results has been taken over all the images, it is more likely that 
the zero values indicate no object within the particular range.

As can be seen in the results, SGBM performs better in finding more accurate corresponding matches 
compared to ADCensus, as most of the error points fall below the standard stereoacuity lines. Moreover, the plots show that in both methods 
the significant amount of error
corresponds to the near field objects, within the first 5 meters. This range of the depth field can be considerably important in some applications,
such as the ones involving certain manipulative tasks.

\subsection{Depth edges and Occlusion}
In order to examine the effect of evaluating certain regions of the disparity image instead of the whole image, 
we estimated the average error both for the masked areas and the whole disparity map. 
Results of SGBM are shown in Figures \ref{fig:sgbmfull3} and \ref{fig:sgbmmsk3}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{sgbmmsk3}
\caption{Average disparity error over masked areas by SGBM}
\label{fig:sgbmmsk3}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{sgbmfull3}
\caption{Average disparity error over the whole image by SGBM}
\label{fig:sgbmfull3}
\end{figure} 

The plots show that the average error over the masked regions, that is near the depth edges, is very similar to the results over the whole image. 
This may imply that there is no additional benefit in the inspection of these regions. 
However, this might be merely an indication of the performance of the selected algorithms and can be better analyzed by evaluating more algorithms 
within our model.
In either case, we argue that, due to the importance of occlusion and areas near depth discontinuities to the HVS in AR applications, 
it is reasonable to focus more on the depth edges and their surroundings when designing or employing a stereo matching technique for an AR application.

\subsection{Average Outliers}
In this experiment the average outliers were measured for both algorithms. 
The values for both validity criteria mentioned in chapter \ref{chap:System}, valid pixels in the ground truth and generated disparity, 
are presented in Tables \ref{tab:outlmsk} and \ref{tab:outlfull} for each age group. For simplicity, we have labelled the valid pixels
in the ground truth and the generated disparity with \textbf{vG} and \textbf{vD}, respectively.
Figure \ref{fig:outlbar} also represents a chart of all the results for one of these validity criteria, when the grount truth disparity is valid.

%{\footnotesize
\begin{minipage}{0.8\linewidth}
\begin{center}
\captionof{table}{Average outliers for the masked regions}
\label{tab:outlmsk}
\begin{tabular}{ |c|c|c|c| }
\hline
Algorithm & Age & Avg\_Outliers(vG) & Avg\_Outliers(vD) \\ \hline
\multirow{4}{*}{SGBM} & 17-29 & 0.12 & 0.16 \\
& 30-49 & 0.11 & 0.15 \\
& 50-69 & 0.09 & 0.12 \\
& 70-83 & 0.0012 & 0.0016 \\ \hline
\multirow{4}{*}{ADCensus} & 17-29 & 0.23 & 0.32 \\
& 30-49 & 0.22 & 0.31 \\
& 50-69 & 0.18 & 0.27 \\
& 70-83 & 0.002 & 0.003 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline \newline
%}

\begin{minipage}{0.8\linewidth}
\begin{center}
\captionof{table}{Average outliers for the whole image}
\label{tab:outlfull}
\begin{tabular}{ |c|c|c|c| }
\hline
Algorithm & Age & Avg\_Outliers(vG) & Avg\_Outliers(vD)  \\ \hline
\multirow{4}{*}{SGBM} & 17-29 & 0.11 & 0.14 \\
& 30-49 & 0.10 & 0.12 \\
& 50-69 & 0.08 & 0.09 \\
& 70-83 & 0.005 & 0.007 \\ \hline
\multirow{4}{*}{ADCensus} & 17-29 & 0.27 & 0.39 \\
& 30-49 & 0.26 & 0.37 \\
& 50-69 & 0.22 & 0.32 \\
& 70-83 & 0.002 & 0.003 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline

Results show that in both cases, the masked regions and the whole image, SGBM has less number of outliers than ADCensus, indicating that
SGBM generates a more accurate disparity map as perceived by the human visual system.
Another observation is that in SGBM, the number of outliers over the masked regions are more than the outliers over the whole image, whereas in ADCensus the
opposite behavior is observed. This implies that SGBM, as a solution per se, generate less accurate results
near the depth discontinuities and occluded regions compared to the other areas in the image.
On the other hand, ADCensus per se, results in more accurate disparity values near the depth edges compared to the other regions in the image and 
tends to preserve the occluded regions. This only indicates that, despite the better performance of SGBM over ADCensus according to the
experimental results, in cases where only one of these solutions is available, it is reasonable to consider this behavior to employ the method
in the right application based on the requirement of the target system for the accuracy of the depth results in different regions; in other
words, it is important to first investigate which regions of the image are more important in the context of the target application.
%whether the accuracy near the depth discontinuities and preserving the occlusion is more important than the
%other regions in the image or not.
For instance, ADCensus performs better in an application where the areas
near the depth discontinuities and occlusion are more important than the rest of the image, such as image compositing for layering visual elements
on the scene, compared to application scenarios where obtaining an accurate, dense disparity map for all the regions in an image is essential.

\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{outlchart}
\caption{Average disparity error for the masked regions - not refined}
\label{fig:outlbar}
\end{figure} 

%Furthermore, in SGBM the number of outliers over the masked regions are more than the outliers over the whole image, whereas in ADCensus the
%opposite behavior is observed; that is the number of outliers over the masked regions are less than 
%the whole image. This implies that ADCensus performs better in the areas of depth edges and their surroundings than other regions.
%Therefore, it is better to be employed in the applications where preserving the object boundaries and occluded areas
%is adequate and more important than the other regions in the image, such as image compositing for layering visual elements
%on the scene. However, this metric per se
%cannot be an indicator of the suitability of the solution for any target application and should be considered along with the other
%relevant metrics. 

We should note that the reason the number of outliers for the valid pixels in the generated disparity is more than the outliers for the valid pixels
in the ground truth is that in our implementation, the counter for the number of pixels in the ground truth image 
is incremented whenever a disparity pixel in the ground truth is labelled as valid, regardless of other conditions in the process; 
however, the counter for the number of pixels in the generated disparity
is only incremented whenever the disparity value is valid and the amount of disparity error is less than the specified pixel threshold in the evaluation process, thus resulting
in a smaller denominator of the fraction in the estimation of the average outliers for the criteria of valid pixels in the generated disparity map and, therefore, a larger 
average value in the end.

\subsection{Average Disparity Error}
The average disparity error have also been estimated in the evaluation process for both pixel validity criteria. However, the resulting values
were similar for both cases and, therefore, only one value is reported in the following table for this metric.

\begin{minipage}{0.8\linewidth}
\begin{center}
\captionof{table}{Average disparity error}
\label{tab:avgerr}
\begin{tabular}{ |c|c|c| }
\hline
Algorithm & Region & Avg\_DispErr \\ \hline
\multirow{2}{*}{SGBM} & Full & 6.58 \\ \cline{2-3}
& Masked & 7.81 \\ \hline
\multirow{2}{*}{ADCensus} & Full & 4.49 \\ \cline{2-3}
& Masked & 4.74 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline

As can be seen, ADCensus results in less average disparity error than SGBM. This difference is likely caused by the various refinement steps
implemented in the ADCensus algorithm which do not exist in SGBM.
As a result, despite the larger number of outliers in ADCensus than SGBM as measured in the previous experiments,
ADCensus attempts to decrease the difference between the resulting disparity value and the ground truth disparity, thus generating a smoother disparity patches
within different regions of the image.

\subsection{Real-time Execution}
In another experiment, we estimated the average execution time for both algorithms. Results show that the average execution time over all the images 
for SGBM and ADCensus are $0.54$ and $272.82$ seconds, respectively.
Considering the requirements of having an interactive real-time AR system \cite{hertz00}, the processing time of each frame should not be more than 0.06-0.08 seconds.
Although the current implementation of SGBM could be used when the real world scene remains stable for approximately one second, it can be safely concluded that
none of these algorithms meet the requirements of a real-time interactive AR system.
This suggests that the GPU-based solutions are better techniques to achieve the processing speed required for the real-time 
interactive applications of AR.

\subsection{Effect of Refinement}
In this experiment, we studied the effect of the post processing steps, also referred to as the \textit{refinement steps}, 
in the stereo algorithms on the accuracy of the results in our evaluation criteria. 

Refinement is usually the last step in a stereo correspondence algorithm because it attempts to decrease the 
number of wrong matches or the error after the disparity results have been found, thus requiring the outliers to be first detected in the results. 
The detection of the outliers occurs through a check known as left-right consistency check in a stereo matching algorithm. In this check, the disparity map for both
the left and right image is first calculated. Then, if a pixel in the left image, based on its disparity value, corresponds to a pixel in the right image
that does not map back to it, it will be labeled as an outlier. This description can be formulated as follows:
\begin{align}
D_{L}(p) \neq D_{R}(p-(D_{L},0))
\end{align}
\noindent
Where $D_{L}(p)$ is the disparity function for the left image and $D_{R}$ is the disparity function for the right image.

For simplicity, we will refer
to this check as LR check in this report.
In our implementation of ADCensus, we have the LR check and its subsequent refinement steps triggered with a flag.
Therefore, when the flag is not set, neither the check nor the refining steps are triggered in the algorithm.
To investigate the effect of the refinement on the final results, we used 
ADcensus in this experiment with the LR flag set to zero, generating the disparity results for the image pairs, and evaluating the results.
The results for both cases, refined and not refined, over the masked regions and the whole image are shown below.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcenmsk3NoLR}
\caption{Average disparity error for the masked regions - not refined}
\label{fig:adcmnoLR}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcenmsk3}
\caption{Average disparity error for the masked regions - refined}
\label{fig:adcm3}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcenfull3NoLR}
\caption{Average disparity error for the whole image - not refined}
\label{fig:adcfnoLR}
\end{figure} 

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{adcenfull3}
\caption{Average disparity error for the whole image - refined}
\label{fig:adcf3}
\end{figure} 

As can be seen in the plots, the evaluation results in our specific criteria 
are not significantly different from the results of the algorithm when LR check and refinement were triggered
and only a few average values have slightly changed. We have marked a few of these values with a blue circle in Figures \ref{fig:adcmnoLR} and \ref{fig:adcfnoLR}.

We also estimated the average execution time, the average disparity error, and the average outliers in this experiment. The results for 
the average error and outliers are shown in the tables below.

\begin{minipage}{0.8\linewidth}
\begin{center}
\captionof{table}{ADCensus average disparity error - no refinement}
\label{tab:adcerrNref}
\begin{tabular}{|c|c|}
\hline
Region & Avg\_DispErr \\ \hline
Masked & 5.59 \\  \hline
Full & 5.29 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline

\begin{minipage}{0.8\linewidth}
\begin{center}
\captionof{table}{ADCensus average outliers - no refinement}
\label{tab:adcoutlNref}
\begin{tabular}{ |c|c|c|c| }
\hline
Region & Age &  Avg\_Outliers(vG) & Avg\_Outliers(vD)  \\ \hline
\multirow{4}{*}{Masked} & 17-29 & 0.23 & 0.33 \\
& 30-49 & 0.22 & 0.31 \\
& 50-69 & 0.18 & 0.27 \\
& 70-83 & 0.002 & 0.003 \\ \hline
\multirow{4}{*}{Full} & 17-29 & 0.27 & 0.39 \\
& 30-49 & 0.26 & 0.37 \\
& 50-69 & 0.23 & 0.32 \\
& 70-83 & 0.001 & 0.002 \\ \hline
\end{tabular}
\end{center}
\end{minipage} \newline

Figure \ref{fig:outlnoref} shows a comparison between the results of ADCensus with and without the refinement 
for one of the validity criteria, when the ground truth disparity is valid. As can be seen, no significant decrease is obtained in the number of outliers.

\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{adcrefNoref}
\caption{Average disparity error for the masked regions - not refined}
\label{fig:outlnoref}
\end{figure} 

The average execution time was approximately $147.84$ seconds which is nearly half the running time of the algorithm with the LR check and refinements triggered. 
Comparing these results to the ones presented in Tables \ref{tab:outlmsk}, \ref{tab:outlfull}, and \ref{tab:avgerr} a slight decrease in the amount of errors and 
almost no change in the number of outliers is observed.
Analyzing the results in this experiment, we can conclude that despite the considerable rise in the execution time of the algorithm, no significant
accuracy is achieved in our evaluation criteria through refinement of the disparity results; therefore, the execution of ADCensus without any LR check and
refinement step is more beneficial to an AR application in outdoor environments.

\section{Hypotheses Validation}
Next, we will review our hypotheses mentioned earlier in this chapter and discuss their validity in the light of our experiments and their results.

\begin{itemize}
\item \textbf{Hypothesis 1}: \emph{Our model evaluates and demonstrates the accuracy of a stereo matching algorithm in the framework of 
outdoor augmented reality applications.} 
As can be seen in the results of the experiments, our system employs a systematic approach for the measurement and demonstration of different 
evaluation metrics in the framework of an outdoor augmented reality application. 
In our system, the disparity error, which is the most important metric for the accuracy of the disparity results, 
is converted to a certain measurement, stereoacuity, that is relevant and applicable to the human visual
system and its perception of depth. We evaluated two stereo matching methods in our system and analyzed their performance in terms of
the accuracy of the depth map for an outdoor AR application. Due to the application-oriented design of the system, we could comment on the suitability of each method
for an AR application in outdoor environments. As a result, we can claim that our evaluation model is more appropriate for the evaluation of the solutions 
in an outdoor AR system than the conventional evaluation systems.

\item \textbf{Hypothesis 2:} \emph{Observing, evaluating, and consequently 
refining the areas near the depth edges in an image are more important in an AR application.}
The results of our experiments on two sample stereo matching solutions showed no significant difference between the evaluation metrics corresponding to the whole image and
the regions of the depth edges and their surroundings, thereby implying that there may be no specific benefit into the analysis of these particular regions 
in an outdoor AR application.
However, due to the importance of these regions as depth cues to the human visual system for the perception of the 3D location of the surrounding 
objects in an environment, we argue that more solutions should be tested within our evaluation model 
to be able to certainly approve or disapprove this hypothesis. 

\item \textbf{Hypothesis 3:} \emph{Our system performs better than the the conventional evaluation models for assessing the performance of a stereo algorithm in
Real-time AR applications.}
As explained in the design of our model and demonstrated in the experimental results, the execution time of the algorithms 
is estimated and evaluated in the system based on the requirements of having a real-time and interactive augmented reality application.
In the experiments, we evaluated the running time of the two sample stereo matching algorithms which proved to be inefficient in both cases for 
a real-time augmented reality application. Through this property, we can claim that the evaluation results through our system is more beneficial to AR applications
than the conventional evaluation models which do not take this important aspect of the solutions into account.

\item \textbf{Hypothesis 4:} \emph{The trade-off between the accuracy and the running time of the stereo algorithms can be effectively evaluated 
in the framework of an outdoor AR application through our system.} 
In one of the experiments, we focused on the trade-off between the accuracy of the results and the running time of the algorithm by studying the effect
of the post processing steps, also referred to as refinement steps, on the evaluation metrics. Results on ADCensus showed that integrating these steps in the algorithm
does not significantly improve the accuracy of the results in the framework of an outdoor AR system; on the other hand, it causes a considerable increase in the execution
time of the algorithm which is not beneficial to a real-time and interactive AR system. The results of this evaluation indicates that the trade-off between the accuracy 
of the results and the execution time of the algorithm, which normally exists in nearly all the stereo matching solutions, can be effectively analyzed 
through our evaluation system. The conventional evaluation models lack this property which is of great importance to certain applications, such as applications of AR.

\end{itemize}

\section{Detectability of Depth in AR}
According to different studies \cite{dras96, kru10,azuma01}, some other factors such as issues associated with the environment, display device, and capturing device
can also affect the perception of depth in the visual system. As a result, the ability to detect the difference in depth 
does not merely depend on how accurate the stereo correspondence algorithm can estimate the depth values and detect the steps in depth.
In order to investigate the validity of this statement, we conducted another experiment. In this experiment we defined some stereoacuity thresholds,
or better to phrase as the detectable depth difference
in terms of stereoacuity. In order to find the minimum threshold to start with, we attempted to find the 
minimum disparity change in the grount truth disparity images. To this end, we move through horizontal scanlines in each image and 
compute the difference between the values of consecutive pixels, which is, in fact, an indicator of the detectability of the difference in depth. 
After finding the minimum value in each image, a global minimum is sought between all the computed values from different images.
The value we found for image size of $1242\times375$ and the focal length of $721$ millimeters was $0.0022$ arcmins.
After finding this minimum and defining our thresholds, we apply a nearly similar operation on the disparity results from each of the sample algorithms.
In this process, while moving through each image, for those pixels whose generated disparity is close to the ground truth disparity, within a pre-defined pixel threshold, 
we estimate their depth difference from their following pixel
in the ground truth and compare the value to each of the specified thresholds; if this value is less than a threshold, then we check to see whether
the stereo algorithm has also detected different values for the corresponding pixels. 
In case of detection, we increment a counter corresponding to each threshold.
This process is repeated for different images and, finally, the average of the detected changes is estimated for each specified threshold.

\begin{alltt}
\textbf{ADP}; START
      define StAc\_thresh;
      for all pixels p in the image; do
         if (\( |disp\sb{gt}-disp\sb{gen}|<pix\_thresh\));
            \(pix\_count++\);
            \(diff = |disp\sb{gen\sb{i}}-disp\sb{gen\sb{i+1}}|\);
            \(depth\sb{gt\sb{i}} = \frac{focal\_length \times baseline}{disp\sb{gen\sb{i}}}\);
            \(depth\sb{gt\sb{i+1}} = \frac{focal\_length \times baseline}{disp\sb{gen\sb{i+1}}}\);
            \(depthDiff = |depth\sb{gt\sb{i}}-depth\sb{gt\sb{i+1}}|\);
            \(stAc\_detected = \frac{pupil\_distance*depthDiff}{depth\sb{gt\sb{i}}\sp{2}}\);
            for each threshold \textit{thr} in StAc\_thresh; do
               if (\(stAc\_detected<thr\))
                  if (\(diff>0\))
                     \(detected[thr]++\);
                  fi
               fi
            end for
      end for
      for each threshold \textit{thr} in StAc\_thresh; do
         \(Avg\_detected[thr] = detected[thr]/pix\_count\);
      end for
\textbf{ADP}; END
\end{alltt}
The results for both algorithms are shown in Figure \ref{fig:algthresh}.

\begin{figure}[H]
\centering
\includegraphics{algthreshBoth}
\caption{Average detected changes for different stereoacuity thresholds}
\label{fig:algthresh}
\end{figure} 

As can be seen in these plots, for both algorithms, the average detected changes in disparity differences starts to converge at the value of approximately $0.4$ arcmins.
We also observe that for the values below this threshold the average detected changes are much lower and for some values, such as the minimum detectable threshold in ground truth,
both algorithms are not capable of detecting any change. 
This implies that, regardless of the accuracy resolution of the algorithms, which is $1/8$th of a pixel for SGBM and $1/16$th of a pixel for ADCensus and approximately equal to
$0.6$ arcmins and $0.3$ arcmins, respectively, for Kitti images based on the camera parameters and the geometrical relation presented in Figure \ref{fig:camResolution} and 
Equation \ref{eq:algResolution}, some changes in 
depth in the real world are still not be detected by the algorithm. This effect might be related to the sensor resolution, that is, the resolution of the capturing device or the effect
of noise. Therefore, in order to achieve better detectability of depth changes by the algorithms, that is, to obtain a lower threshold
which is closer to the actual resolution of the implemented algorithm, using higher resolution devices is also essential.
However, our previous experimental results show that despite various types of error caused by the capturing device, noise, or the discretization limit of the stereo correspondence 
algorithm in the estimation of disparity values, in many cases depending on the distance of the object from the observer,  
the error in the results will not be perceptible to the HVS in an outdoor AR
application.

\begin{figure}[H]
\centering
\includegraphics{camRes}
\caption{Resolution of image in angular disparity}
\label{fig:camResolution}
\end{figure}
\noindent
Where, $w$ is the image width and $f$ is the focal length of the capturing device.
For the imge size of $1242\times375$ and focal length of $721$, and based on the resolution of SGBM and ADCensus in the estimation of the disparity values, 
the detectable disparity difference in terms of stereoacuity is as follows:

\begin{align}
\label{eq:algResolution}
SGBM: \theta &= \arctan (\frac{\frac{1}{8}}{721}) = 0.0099 degrees \times 60 \frac{arcmins}{dgrees} = 0.59 arcmins \\
ADCensus: \theta &= \arctan (\frac{\frac{1}{16}}{721}) = 0.0049 degrees \times 60 \frac{arcmins}{degrees}= 0.29 arcmins
\end{align}
